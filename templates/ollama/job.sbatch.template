#!/bin/bash
#SBATCH --partition={{PARTITION}}
#SBATCH --nodes={{NODES}}
#SBATCH --ntasks={{NTASKS}}
#SBATCH --cpus-per-task={{CPUS_PER_TASK}}
#SBATCH --mem={{MEM}}
#SBATCH --job-name={{JOB_NAME}}
#SBATCH --output={{STDOUT_FILE}}
#SBATCH --error={{STDERR_FILE}}
#SBATCH --time={{TIME}}
#SBATCH --mail-user={{MAIL_USER}}
#SBATCH --mail-type={{MAIL_TYPE}}
#SBATCH --chdir={{CHDIR}}

set -euo pipefail

head -n 20 /proc/cpuinfo
grep "processor" /proc/cpuinfo | wc -l
cat /proc/meminfo

CONTAINER_IMAGE="{{CONTAINER_IMAGE}}"
MODEL_PATH="{{MODEL_PATH}}"
PORT={{PORT}}
STARTUP_SLEEP={{STARTUP_SLEEP}}

VENV_DIR="{{VENV_DIR}}"
REQ_FILE="{{REQUIREMENTS_FILE}}"
CLIENT_SCRIPT="{{CLIENT_SCRIPT}}"

INSTANCE_NAME="ollama-${USER}-${SLURM_JOB_ID:-manual}"

unset ROCR_VISIBLE_DEVICES

mkdir -p "{{STDOUT_DIR}}" "{{STDERR_DIR}}" "{{RUNS_DIR}}"

cleanup() {
  apptainer instance stop "$INSTANCE_NAME" >/dev/null 2>&1 || true
}
trap cleanup EXIT

apptainer instance start \
  --nv \
  --writable-tmpfs \
  --bind "$MODEL_PATH" \
  "$CONTAINER_IMAGE" "$INSTANCE_NAME"

apptainer exec instance://$INSTANCE_NAME \
  bash -c "export OLLAMA_MODELS=$MODEL_PATH && ollama serve &"
echo "ðŸ¦™ Ollama is now serving at http://localhost:$PORT"

sleep "$STARTUP_SLEEP"

apptainer exec instance://$INSTANCE_NAME \
  bash -c "ollama list"

module load {{MODULE_CUDA}}
module load {{MODULE_PYTHON}}

python -m venv "$VENV_DIR"
source "$VENV_DIR/bin/activate"
python --version

pip install --upgrade pip
pip install -r "$REQ_FILE"

python "$CLIENT_SCRIPT"