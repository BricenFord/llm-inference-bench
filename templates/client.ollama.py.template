#!/usr/bin/env python3
import time
import requests

HOST = "{{HOST}}"
MODEL_NAME = "{{MODEL_NAME}}"
CSV_FILE = "{{CSV_FILE}}"
TIMEOUT_S = {{TIMEOUT_S}}
LOG_QA = {{LOG_QA}}
MAX_QUESTIONS = {{MAX_QUESTIONS}}

def load_questions(file_path):
    questions = []
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            s = line.strip()
            if s:
                questions.append(s)
    return questions

def ask_model(prompt):
    payload = {"model": MODEL_NAME, "prompt": prompt, "stream": False}
    start = time.time()
    resp = requests.post(f"{HOST}/api/generate", json=payload, timeout=TIMEOUT_S)
    resp.raise_for_status()
    end = time.time()
    answer = resp.json().get("response", "").strip()
    return answer, (end - start)

def main():
    questions = load_questions(CSV_FILE)
    if MAX_QUESTIONS and MAX_QUESTIONS > 0:
        questions = questions[:MAX_QUESTIONS]

    total_questions = len(questions)
    start_total = time.time()

    for i, q in enumerate(questions, start=1):
        try:
            answer, latency = ask_model(q)
        except Exception as e:
            print(f"{i}, ERROR, {e}", flush=True)
            continue

        elapsed_total = time.time() - start_total
        throughput = i / elapsed_total if elapsed_total > 0 else 0.0

        if LOG_QA:
            print(
                f"{i}, {latency:.3f}s, {throughput:.3f} q/s, "
                f"QUESTION={q} | ANSWER={answer}",
                flush=True
            )
        else:
            print(f"{i}, {latency:.3f}s, {throughput:.3f} q/s,", flush=True)

    total_time = time.time() - start_total
    overall_throughput = (total_questions / total_time) if total_time > 0 else 0.0

    print(f"\nTotal questions: {total_questions}", flush=True)
    print(f"Total time: {total_time:.2f} seconds", flush=True)
    print(f"Overall throughput: {overall_throughput:.2f} questions/sec", flush=True)

if __name__ == "__main__":
    main()
